{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning이란?\n",
    "- 머신러닝의 한 종류\n",
    "- 여러 개의 층(은닉층)을 가진 신경망을 사용해 머신러닝을 수행하는 것\n",
    "- 이미지 인식, 음성 인식, 자연어 처리 등의 다양한 분야에 활용\n",
    "- 1980년대부터 있었지만 현대에 와서 컴퓨터 성능이 좋아지고 비즈니스적으로 성공하면서 주목받기 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning과 Machine Learning의 차이점\n",
    "- 가장 큰 차이점: Feature 추출 - 딥러닝은 Feature를 자동으로 추출한다.\n",
    "- Deep Learning은 스스로 학습을 하기 때문에 Machine Learning보다 많은 Data가 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강인공지능 - 아직 이론도 나온 것이 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제)스마트폰을 구매할까?\n",
    "#### 아래의 이진분류가 있다고 하자\n",
    "- 구매한다: y = 1\n",
    "- 안 산다: y = 0\n",
    "\n",
    "##### 2진분류 > 시그모이드가 일단 있음\n",
    "\n",
    "### 입력의 요인들 정리\n",
    "- X1: 이번 달의 수입은 충분한가?\n",
    "- X2: 최신 기능을 가지고 있는가?\n",
    "- X3: 기존의 스마트폰에 문제가 있는가?\n",
    "\n",
    "##### x1, x2, x3의 가중치를 w1, w2, w3으로 하였을 때 예상되는 조건들은 다음과 같다\n",
    "- Case1: 엄청부자: w2가 가장 중요, w3 별로 안 중요, w1은 아예 생각 안함 (w1= 1, w2=8, w3=2)\n",
    "- Case2: 스마트폰 당장 고장난 사람: w3=8, w1=2, w2=1 - 스마트폰이 최신 기능이 있는지는 안 중요하니깐\n",
    "- Case3: 정기적으로 구매하는 사람: w1=1, w2=8, w3=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 용어정리\n",
    "#### 순전파(foward propagation)\n",
    "- 왼쪽에서 오른쪽으로 흘러가는 과정\n",
    "- 순전파에 의해 딥러닝의 출력값(y^)이 결정. (정답은 y)\n",
    "\n",
    "#### 손실함수(Loss Function)\n",
    "- 출력값과 정답의 차이\n",
    "- 출력값과 덩답이 일치할 수록 손실함수의 값은 작아진다.\n",
    "- 회귀에서는 평균제곱오차(Mean Squared Error)를 사용\n",
    "- 분류에서는 크로스 엔트로피(Cross Entropy)를 사용\n",
    "- 매개변수(w, b)를 조절해서 손실함수의 값을 최저로 만드는 과정을 최적화(Optimization)이라고 한다\n",
    "- 최적화 과정은 Optimizer를 통해 이루어지며 Optimizer는 역전파(Back Propagation)과정을 수행해서 딥러닝 모델의 매개변수를 최적화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화(Optimization)\n",
    "- 대표적인 방법은 경사하강법\n",
    "- 반복적으로 손실함수에 대한 모델 매개변수의 기울기를 구한 후 미분값의 반대방향으로 매개변수를 조절해 나가면 결국에는 최저 손실함수에 도달한다.\n",
    "- 가장 낮은 지점(기울기가 0이 되는 지점)을 미분을 통해 구한다.\n",
    "\n",
    "### 역전파(Backward Propagation)\n",
    "- Optimizer는 손실함수의 값을 최소화하기 위해 역전파를 사요해 딥러닝 모델의 모든 매개변수를 변경한다.\n",
    "- 손실함수의 값을 최소화한다는 것은 정답과 예측값의 차이를 최소화하는 것이며, 에러율을 최저로 줄인다는 의미다.\n",
    "- 최신의 w와 b를 가지고 에포크를 수행하는 것이 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝의 과대적합(overfitting)과 그 해결\n",
    "### 드롭 아웃(drop out)\n",
    "- 매개변수 중 일정량을 학습 중간마다 무작위로 빼고 학습하는 방법(너무 잘 맞춘다 - 빼고 해봄)\n",
    "- 드롭아웃을 사용하면 모델에 앙상블 효과를 준다.\n",
    "### 조기 종료(Early Stopping)\n",
    "- 무조건 학습 반복 횟수를 높이면 학습시간이 길어져서 학습데이터만 성능이 좋은 현상이 발생\n",
    "- 학습데이터로만 모델의 매개변수를 조정하고 검증데이터로 모델의 정확도를 측정한다.\n",
    "- 조기종료는 학습횟수에 따라 검증 정확도가 꾸준히 떨어지는 시점이 발견되면 그 즉시 학습을 중단하고 최고점을 사용한다는 개념\n",
    "- 딥러닝의 학습을 중간중간 파일로 저장한다(학습된 w, b들을)\n",
    "- 떨어졌다가 다시 더 올라갈 수도 있기 때문에 이것저것 시도해보는 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c34e8390e776d2ee205b71ed5a6130fee3cef8da5e87e926ce18e14f4a070d72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
