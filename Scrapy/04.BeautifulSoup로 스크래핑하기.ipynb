{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "- 간단하게 HTML과 XML에서 정보를 추출\n",
    "- Anaconda에 기본적으로 설치되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>스크레이핑이란?</h1>\n",
      "<h1>스크레이핑이란?</h1>\n",
      "[<p>웹 페이지를 분석하는 것</p>, <p>원하는 부분을 추출하는 것</p>]\n",
      "웹 페이지를 분석하는 것\n",
      "원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# BeautifulSoup을 사용하여 웹 페이지의 내용을 추출하기\n",
    "# import bs4 and request modules\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# Site Address\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam01.html\"\n",
    "\n",
    "# Get the HTML from the site\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup.prettify())\n",
    "\n",
    "# 원하는 부분 찾아 추출하기\n",
    "specific = soup.html.body.h1\n",
    "print(specific)\n",
    "\n",
    "# Find the tag\n",
    "h1 = soup.find(\"h1\")\n",
    "print(h1)\n",
    "\n",
    "# Find every tag\n",
    "ps = soup.find_all(\"p\")\n",
    "print(ps)\n",
    "\n",
    "# Find every tag and get the text\n",
    "ps = soup.find_all(\"p\")\n",
    "for p in ps:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id로 요소 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: 스크레이핑이란?\n",
      "body: 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam02.html\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# id가 title인 항목 추출하기\n",
    "title = soup.find(id='title')\n",
    "print('title:', title.text)\n",
    "\n",
    "# id가 body인 항목 추출하기\n",
    "body = soup.find(id='body')\n",
    "print('body:', body.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 글자, 링크(a, href) 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <ul>\n",
      "   <li>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "   </li>\n",
      "   <li>\n",
      "    <a href=\"http://www.daum.net\">\n",
      "     daum\n",
      "    </a>\n",
      "   </li>\n",
      "  </ul>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam03.html\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver -> http://www.naver.com\n",
      "daum -> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")\n",
    "for link in links:\n",
    "    text = link.text\n",
    "    href = link.get(\"href\")\n",
    "    # href = link.attrs[\"href\"]\n",
    "    \n",
    "    # print text and href\n",
    "    print(text,\"->\", href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(강수) 27일(월) 오후~30일(목) 오전은 흐리고 비가 오겠습니다. \n",
      "(기온) 이번 예보기간 아침 기온은 21~24도로 어제(23일, 아침최저기온 19~23도)와 비슷하거나 조금 높겠고, 낮 기온은 26~30도로 어제(낮최고기온 25~30도)와 비슷하겠습니다. \n",
      "(해상) 서해중부해상의 물결은 27일(월)~30일(목)은 1.0~3.0m로 높게 일겠고, 그 밖의 날은 1.0~2.0m로 일겠습니다.\n",
      "\n",
      "* 이번 예보기간에는 정체전선의 위치에 따라 강수 구역이 변동될 수 있으며, 정체전선의 영향권에서 벗어난 기간에도 대기불안정으로 소나기가 자주 내리는 곳이 있겠으니, \n",
      "  앞으로 발표되는 예보와 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# 날씨정보 가져오기\n",
    "import urllib.request as req\n",
    "import urllib.parse as parse\n",
    "\n",
    "# 기상청 RSS\n",
    "API = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "# 매개변수를 URL 인코딩하기\n",
    "values = {'stnId': '109'}\n",
    "params = parse.urlencode(values)\n",
    "\n",
    "# URL 완성하기\n",
    "url = API + \"?\" + params\n",
    "\n",
    "# 요청 전송하기\n",
    "data = req.urlopen(url).read()\n",
    "text = data.decode('utf-8')\n",
    "\n",
    "# xml 파싱하기\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "# get title and wf\n",
    "title = soup.find('title').string\n",
    "wf = soup.find('wf').string\n",
    "\n",
    "# remove br open and close tag\n",
    "wf = wf.replace(\"<br />\", \"\\n\")\n",
    "wf = wf.replace(\"○ \",\"\")\n",
    "\n",
    "# print title and wf\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS 선택자 사용하기\n",
    "- BeautifulSoup는 자바스크립트 라이브러리인 Jquery처럼 CSS선택자를 지정해서 원하는 요소를 추출가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <div id=\"meigen\">\n",
      "   <h1>\n",
      "    위키북스 도서\n",
      "   </h1>\n",
      "   <ul class=\"items\">\n",
      "    <li>\n",
      "     유니티 게임 이펙트 입문\n",
      "    </li>\n",
      "    <li>\n",
      "     스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "    </li>\n",
      "    <li>\n",
      "     모던 웹사이트 디자인의 정석\n",
      "    </li>\n",
      "   </ul>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam04.html\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위키북스 도서\n"
     ]
    }
   ],
   "source": [
    "# 필요한 부분을 CSS 쿼리로 추출학기(#:id, .:class, [속성]:attr, >:child, ~:sibling, +:adjacent, *:descendant, ~:general sibling,)\n",
    "\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니티 게임 이펙트 입문\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "items = soup.select(\"div#meigen li\")\n",
    "for item in items:\n",
    "    print(item.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 네이버 금융에서 환율 정보 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,297.50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "# 요청 전송하기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 개발자 도구에서 찍기 > copy > copy selector\n",
    "rate = soup.select_one(\"#exchangeList > li.on > a.head.usd > div > span.value\").string\n",
    "\n",
    "print(rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    ": 미국, 일본, 유럽연합, 중국의 환율 가져와 표시하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 USD \t:   1,296.00\n",
      "일본 JPY(100엔) \t:     963.82\n",
      "유럽연합 EUR \t:   1,363.78\n",
      "중국 CNY \t:     193.61\n",
      "달러/일본 엔 \t:   134.4500\n",
      "유로/달러 \t:     1.0528\n",
      "영국 파운드/달러 \t:     1.2275\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "# 요청 전송하기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "names = soup.select('.market_data h3.h_lst > span.blind')\n",
    "rates = soup.select('span.value')\n",
    "for idx, rate in enumerate(rates):\n",
    "    print(names[idx].string,\"\\t:\", rate.string.rjust(10))\n",
    "    if(idx > 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음 영화 랭킹 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 : 스파이더맨: 노 웨이 홈\n",
      " 2 : 모가디슈\n",
      " 3 : 이터널스\n",
      " 4 : 블랙 위도우\n",
      " 5 : 분노의 질주: 더 얼티메이트\n",
      " 6 : 싱크홀\n",
      " 7 : 극장판 귀멸의 칼날: 무한열차편\n",
      " 8 : 베놈 2: 렛 데어 비 카니지\n",
      " 9 : 소울\n",
      "10 : 크루엘라\n",
      "11 : 샹치와 텐 링즈의 전설\n",
      "12 : 인질\n",
      "13 : 듄\n",
      "14 : 보이스\n",
      "15 : 007 노 타임 투 다이\n",
      "16 : 미나리\n",
      "17 : 발신제한\n",
      "18 : 보스 베이비 2\n",
      "19 : 콰이어트 플레이스 2\n",
      "20 : 랑종\n",
      "21 : 유체이탈자\n",
      "22 : 컨저링3: 악마가 시켰다\n",
      "23 : 기적\n",
      "24 : 고질라 VS. 콩\n",
      "25 : 킹스맨: 퍼스트 에이전트\n",
      "26 : 엔칸토: 마법의 세계\n",
      "27 : 연애 빠진 로맨스\n",
      "28 : 장르만 로맨스\n",
      "29 : 미션 파서블\n",
      "30 : 더 수어사이드 스쿼드\n",
      "31 : 비와 당신의 이야기\n",
      "32 : 서복\n",
      "33 : 킬러의 보디가드 2\n",
      "34 : 루카\n",
      "35 : 자산어보\n",
      "36 : 내일의 기억\n",
      "37 : 라야와 마지막 드래곤\n",
      "38 : 프리 가이 \n",
      "39 : 더 스파이\n",
      "40 : 강릉\n",
      "41 : 정글 크루즈\n",
      "42 : 명탐정 코난: 비색의 탄환\n",
      "43 : 캐시트럭\n",
      "44 : 크루즈 패밀리: 뉴 에이지\n",
      "45 : 이스케이프 룸 2: 노 웨이 아웃\n",
      "46 : 극장판 포켓몬스터: 정글의 아이, 코코\n",
      "47 : 극장판 짱구는 못말려: 격돌! 낙서왕국과 얼추 네 명의 용사들\n",
      "48 : 매트릭스: 리저렉션\n",
      "49 : 방법: 재차의\n",
      "50 : 새해전야\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "\n",
    "# 요청 전송하기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "names = soup.select('.link_txt')\n",
    "for idx, name in enumerate(names):\n",
    "    print(\"%2d\"%(idx+1),\":\", name.string)\n",
    "    if(idx > 48):\n",
    "        break\n",
    "\n",
    "images = soup.select('div.poster_movie > img')\n",
    "for idx, image in enumerate(images):\n",
    "\n",
    "    # 이미지 다운로드하기\n",
    "    url = image.get('src')\n",
    "    filename = f'posters/{str(idx+1)}-{names[idx].string}.jpg'\n",
    "    req.urlretrieve(url, filename)\n",
    "\n",
    "    if(idx > 48):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c34e8390e776d2ee205b71ed5a6130fee3cef8da5e87e926ce18e14f4a070d72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
